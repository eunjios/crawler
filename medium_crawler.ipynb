{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "import urllib\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def get_soup(source_address, timeout=10):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(source_address, headers=hdr)\n",
    "        source = urllib.request.urlopen(req, timeout=timeout).read()\n",
    "        soup = bs.BeautifulSoup(source, 'lxml')\n",
    "        return soup\n",
    "    # 디버깅을 위해 추가 \n",
    "    except urllib.error.URLError as e:\n",
    "        print('Could not complete urllib request.')\n",
    "        print(f\"Error: {e.reason}\")\n",
    "        traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print('Could not create soup.')\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일 읽어오기\n",
    "def read_pickle(file):\n",
    "    with open(file, 'rb') as handle:\n",
    "         load = pickle.load(handle)\n",
    "    return load\n",
    "\n",
    "# pickle 파일로 저장 \n",
    "def to_pickle(var, dir):\n",
    "    directory = dir + '.pickle'\n",
    "    with open(directory, 'wb') as handle:\n",
    "        pickle.dump(var, handle)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크 불러오기 \n",
    "def get_links(query):\n",
    "    # 검색어 띄어쓰기 처리 \n",
    "    query = query.replace(' ', '%20')\n",
    "    url = 'https://medium.com/search?q=' + query\n",
    "\n",
    "    # TODO: chromedrive 위치 지정  \n",
    "    path = \"/Users/leeeunji/workspace_selenium/chromedriver\"\n",
    "    driver = webdriver.Chrome(path)\n",
    "    driver.get(url)\n",
    "    # action = ActionChains(driver)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 페이지 로드하는 버튼을 클릭하여 다음 포스트 불러오기 \n",
    "    # TODO: range 변경하여 버튼 몇 번 클릭할지 지정\n",
    "    for i in range(1, 10):\n",
    "        try:\n",
    "            xpath = f'//*[@id=\"root\"]/div/div[3]/div[2]/div/main/div/div/div[2]/div[{10*i}]/div/div/button'\n",
    "            element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            element.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            print('Failed to click the button.')\n",
    "            break\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    # 현재 페이지 가져옴 \n",
    "    source = driver.page_source\n",
    "    soup = bs.BeautifulSoup(source, 'lxml')\n",
    "    # TODO: 가져올 링크 주소를 가져올 태그의 요소로 지정\n",
    "    #       medium 에서는 <a aria-label=\"Post Preview Title\" href=\"우리가 가져올 주소\">\n",
    "    #       이렇게 되어 있어서 아래와 같이 가져온 것임 \n",
    "    a_tags = soup.find_all('a', attrs={'aria-label': 'Post Preview Title'})\n",
    "    links = []\n",
    "    for a in a_tags:\n",
    "        try:\n",
    "            # medium 에서는 href 가 상대주소로 지정되어 있음 -> https://medium.com 앞에 추가 \n",
    "            href = urllib.parse.quote(a['href'], safe=':/?&=')  # URL 인코딩\n",
    "            full_url = \"https://medium.com\" + href\n",
    "            # print(full_url) # URL 잘 받아왔는지 확인\n",
    "            links.append(full_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL: {a['href']}\")\n",
    "            print(e)\n",
    "        \n",
    "    driver.close()\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(links):\n",
    "    stories = []\n",
    "    # 우리가 구해놓은 URL로부터 본문 가져옴 \n",
    "    for index, link in enumerate(links):\n",
    "        try:\n",
    "            # print(f'link: {link}') # 링크 잘 받았는지 확인\n",
    "            soup = get_soup(link)\n",
    "            # TODO: 링크의 본문을 가져올 태그와 클래스 지정\n",
    "            #       medium 에서는 <p class=\"pw-post-body-paragraph ~~~\"> 이 본문 \n",
    "            #       아래와 같이 클래스에 'pw-post-body-paragraph'이 포함되면 가져옴  \n",
    "            p_tags = soup.find_all('p', class_=lambda value: value and 'pw-post-body-paragraph' in value.split())\n",
    "            # print(p_tags) # 태그들 잘 가져왔는지 확인\n",
    "            story = ' '.join([p.get_text(strip=True) for p in p_tags])\n",
    "            # print('Story:', story) # 태그들을 하나로 잘 뭉쳤는지 확인 \n",
    "            stories.append(story)\n",
    "            print(index + 1, 'of', len(links), 'stories') # 진행 상황 표시\n",
    "        except:\n",
    "            pass\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 검색어 지정 (여러 개 리스트 형식으로)\n",
    "# queries = ['개념', '기술', '파이썬', '자바', '인공지능', '깃허브']\n",
    "queries = ['음식', '운동']\n",
    "story_dic = dict.fromkeys(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    print(' ::', query, ':: ')\n",
    "    links = get_links(query)\n",
    "    stories = get_stories(links)\n",
    "    to_pickle(stories, 'data/medium_stories/' + query.replace(' ', '_'))\n",
    "    story_dic[query] = stories\n",
    "\n",
    "df = pd.DataFrame({col: pd.Series(stories) for col, stories in story_dic.items()})\n",
    "df.columns = [col.replace(' ', '_') for col in df.columns]\n",
    "# TODO: output 파일명 지정\n",
    "df.to_csv('output_medium.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[커리어 노트 24]에서 자존감을 위한 마음 정리에 대해 얘기를 나누었다. 감정을 구체화해서 정신의 실체를 만나야 한다는데… 자신을 만나야 위로도 하고 치유도 한다는데… 다 좋은 말인 건 알겠는데 대체 어떻게 하라는 건지… 나 또한 답을 몰라 오랫동안답답하기만 했었다. 신체 건강을 위해서 우리는 어떤 일들을 하는지 살펴보자. - 주기적으로 건강검진을 받는다. 예방하고 초기에 발견하기 위해서다. - 아프면 병원에 간다. 전문가의 진단과 치료를 받기 위해서다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: 간단하게 확인하고 싶은 pickle 파일의 파일명 지정\n",
    "lst = read_pickle('data/medium_stories/운동.pickle')\n",
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
